PS E:\final_final_final> & E:/anaconda/envs/torch/python.exe e:/final_final_final/main.py
Loading data...
Loading data from 17000datas.csv...
Loaded 37268 rows from 17000datas.csv
Columns: ['comment', 'toxicity_level', 'insult', 'profanity', 'threat', 'identity_hate']

Toxicity level distribution:
  Level 0: 12952 examples (34.8%)
  Level 1: 12951 examples (34.8%)
  Level 2: 11365 examples (30.5%)

Category distribution:
  insult: 10141 positive examples (27.2%)
  profanity: 10205 positive examples (27.4%)
  threat: 6372 positive examples (17.1%)
  identity_hate: 5332 positive examples (14.3%)
Split data into 22360 training, 7454 validation, and 7454 test examples
Extracting toxicity features...
Adding fixed alphabet with 69 characters
After adding fixed alphabet: 71 characters
Extending vocabulary from training data...
Added 32 new characters from training data
Final vocabulary size: 103 characters
Character coverage: nan% of all character occurrences
Detecting languages for texts...
Language distribution: {'tl': 7453, 'en': 14907}
Extracting toxicity features...
Detecting languages for texts...
Language distribution: {'en': 4950, 'tl': 2504}
Extracting toxicity features...
Detecting languages for texts...
Language distribution: {'tl': 2551, 'en': 4903}

Training model...
Training model for 30 epochs with learning rate 0.0005
E:\anaconda\envs\torch\lib\site-packages\torch\optim\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access th
the learning rate.
  warnings.warn(
Epoch 1/30 - Train Loss: 0.8346, Val Loss: 1.0564, Val Toxicity Acc: 0.6614, Val Category F1: 0.6468, Time: 241.86s
New best model found at epoch 1
Epoch 2/30 - Train Loss: 0.4921, Val Loss: 0.6928, Val Toxicity Acc: 0.8414, Val Category F1: 0.7392, Time: 243.16s
New best model found at epoch 2
Epoch 3/30 - Train Loss: 0.4007, Val Loss: 0.5521, Val Toxicity Acc: 0.9037, Val Category F1: 0.8363, Time: 243.96s
New best model found at epoch 3
Epoch 4/30 - Train Loss: 0.3650, Val Loss: 0.5257, Val Toxicity Acc: 0.9058, Val Category F1: 0.8356, Time: 241.06s
New best model found at epoch 4
Epoch 5/30 - Train Loss: 0.3438, Val Loss: 0.4615, Val Toxicity Acc: 0.9379, Val Category F1: 0.8566, Time: 229.51s
New best model found at epoch 5
Epoch 6/30 - Train Loss: 0.3259, Val Loss: 0.5768, Val Toxicity Acc: 0.8923, Val Category F1: 0.7856, Time: 237.25s
Validation loss did not improve. Patience: 1/5
Epoch 7/30 - Train Loss: 0.3071, Val Loss: 0.4405, Val Toxicity Acc: 0.9269, Val Category F1: 0.8535, Time: 231.59s
New best model found at epoch 7
Epoch 8/30 - Train Loss: 0.3009, Val Loss: 0.5200, Val Toxicity Acc: 0.8770, Val Category F1: 0.8255, Time: 230.65s
Validation loss did not improve. Patience: 1/5
Epoch 9/30 - Train Loss: 0.2938, Val Loss: 0.4489, Val Toxicity Acc: 0.9313, Val Category F1: 0.8521, Time: 230.07s
Validation loss did not improve. Patience: 2/5
Epoch 10/30 - Train Loss: 0.2804, Val Loss: 0.4753, Val Toxicity Acc: 0.9119, Val Category F1: 0.8475, Time: 231.03s
Validation loss did not improve. Patience: 3/5
Epoch 11/30 - Train Loss: 0.2785, Val Loss: 0.7844, Val Toxicity Acc: 0.7022, Val Category F1: 0.7531, Time: 230.69s
Validation loss did not improve. Patience: 4/5
Epoch 12/30 - Train Loss: 0.2344, Val Loss: 0.5604, Val Toxicity Acc: 0.8401, Val Category F1: 0.8193, Time: 237.71s
Validation loss did not improve. Patience: 5/5
Early stopping triggered at epoch 12
Loading best model from epoch 7

Evaluating on test set...

Toxicity Classification Report:
Accuracy: 0.9288

Confusion Matrix (rows=true, columns=predicted):
              | Pred Not Toxic | Pred Toxic | Pred Very Toxic |
--------------+----------------+------------+-----------------|
True Not Toxic |           2376 |        209 |               6 |
True Toxic     |              6 |       2539 |              45 |
True Very Toxic|              0 |        265 |            2008 |

Toxicity Class Metrics:
  Not Toxic:
    Precision: 0.9975
    Recall: 0.9170
    F1-score: 0.9556
  Toxic:
    Precision: 0.8427
    Recall: 0.9803
    F1-score: 0.9063
  Very Toxic:
    Precision: 0.9752
    Recall: 0.8834
    F1-score: 0.9271

Category Metrics:
  Insult:
    Precision: 0.6585
    Recall: 0.9798
    F1-score: 0.7876
  Profanity:
    Precision: 0.6866
    Recall: 0.9775
    F1-score: 0.8066
  Threat:
    Precision: 0.9725
    Recall: 0.9123
    F1-score: 0.9414
  Identity_hate:
    Precision: 0.9553
    Recall: 0.8026
    F1-score: 0.8723

Category Macro-Average F1: 0.8520
Creating OOD test set with criteria: long_texts
Selected 1047 texts longer than 426.0 characters
Saved OOD test set with 500 examples to output\ood_test_data.csv

Performing comprehensive evaluation...
Evaluating on in-distribution data...
Extracting toxicity features...

Toxicity Classification Report:
Accuracy: 0.9405

Confusion Matrix (rows=true, columns=predicted):
              | Pred Not Toxic | Pred Toxic | Pred Very Toxic |
--------------+----------------+------------+-----------------|
True Not Toxic |          11928 |       1006 |              18 |
True Toxic     |             11 |      12788 |             152 |
True Very Toxic|              3 |       1026 |           10336 |

Toxicity Class Metrics:
  Not Toxic:
    Precision: 0.9988
    Recall: 0.9209
    F1-score: 0.9583
  Toxic:
    Precision: 0.8629
    Recall: 0.9874
    F1-score: 0.9210
  Very Toxic:
    Precision: 0.9838
    Recall: 0.9095
    F1-score: 0.9452

Category Metrics:
  Insult:
    Precision: 0.6723
    Recall: 0.9854
    F1-score: 0.7992
  Profanity:
    Precision: 0.6994
    Recall: 0.9837
    F1-score: 0.8175
  Threat:
    Precision: 0.9825
    Recall: 0.9154
    F1-score: 0.9478
  Identity_hate:
    Precision: 0.9681
    Recall: 0.8537
    F1-score: 0.9073

Category Macro-Average F1: 0.8680

Evaluating on out-of-distribution data...
Extracting toxicity features...
E:\anaconda\envs\torch\lib\site-packages\sklearn\metrics\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
E:\anaconda\envs\torch\lib\site-packages\sklearn\metrics\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
E:\anaconda\envs\torch\lib\site-packages\sklearn\metrics\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

Toxicity Classification Report:
Accuracy: 0.8480

Confusion Matrix (rows=true, columns=predicted):
              | Pred Not Toxic | Pred Toxic | Pred Very Toxic |
--------------+----------------+------------+-----------------|
True Not Toxic |              0 |          5 |               0 |
True Toxic     |              0 |        420 |              17 |
True Very Toxic|              0 |         54 |               4 |

Toxicity Class Metrics:
  Not Toxic:
    Precision: 0.0000
    Recall: 0.0000
    F1-score: 0.0000
  Toxic:
    Precision: 0.8768
    Recall: 0.9611
    F1-score: 0.9170
  Very Toxic:
    Precision: 0.1905
    Recall: 0.0690
    F1-score: 0.1013

Category Metrics:
  Insult:
    Precision: 0.7295
    Recall: 0.9780
    F1-score: 0.8357
  Profanity:
    Precision: 0.6931
    Recall: 0.9765
    F1-score: 0.8107
  Threat:
    Precision: 0.6364
    Recall: 0.1346
    F1-score: 0.2222
  Identity_hate:
    Precision: 0.1250
    Recall: 0.0541
    F1-score: 0.0755

Category Macro-Average F1: 0.4860

Performance Gap Analysis:
In-distribution accuracy: 0.9405
Out-of-distribution accuracy: 0.8480
Gap: 0.0925 (9.8% drop)

Performing subgroup analysis...

Evaluating on texts with length 0-50 (13187 examples)
Extracting toxicity features...

Evaluating on texts with length 51-100 (17168 examples)
Extracting toxicity features...

Evaluating on texts with length 101-200 (4436 examples)
Extracting toxicity features...

Evaluating on texts with length 201-300 (938 examples)
Extracting toxicity features...
E:\anaconda\envs\torch\lib\site-packages\sklearn\metrics\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
E:\anaconda\envs\torch\lib\site-packages\sklearn\metrics\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
E:\anaconda\envs\torch\lib\site-packages\sklearn\metrics\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

Evaluating on texts with length 301-inf (1539 examples)
Extracting toxicity features...
E:\anaconda\envs\torch\lib\site-packages\sklearn\metrics\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
E:\anaconda\envs\torch\lib\site-packages\sklearn\metrics\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
E:\anaconda\envs\torch\lib\site-packages\sklearn\metrics\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

Evaluating on tl texts (12507 examples)
Extracting toxicity features...

Evaluating on en texts (24761 examples)
Extracting toxicity features...

Evaluating on not_toxic texts (12952 examples)
Extracting toxicity features...
E:\anaconda\envs\torch\lib\site-packages\sklearn\metrics\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
E:\anaconda\envs\torch\lib\site-packages\sklearn\metrics\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
E:\anaconda\envs\torch\lib\site-packages\sklearn\metrics\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
E:\anaconda\envs\torch\lib\site-packages\sklearn\metrics\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
E:\anaconda\envs\torch\lib\site-packages\sklearn\metrics\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
E:\anaconda\envs\torch\lib\site-packages\sklearn\metrics\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

Evaluating on toxic texts (12951 examples)
Extracting toxicity features...
E:\anaconda\envs\torch\lib\site-packages\sklearn\metrics\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
E:\anaconda\envs\torch\lib\site-packages\sklearn\metrics\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
E:\anaconda\envs\torch\lib\site-packages\sklearn\metrics\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
E:\anaconda\envs\torch\lib\site-packages\sklearn\metrics\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
E:\anaconda\envs\torch\lib\site-packages\sklearn\metrics\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
E:\anaconda\envs\torch\lib\site-packages\sklearn\metrics\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

Evaluating on very_toxic texts (11365 examples)
Extracting toxicity features...
E:\anaconda\envs\torch\lib\site-packages\sklearn\metrics\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
E:\anaconda\envs\torch\lib\site-packages\sklearn\metrics\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
E:\anaconda\envs\torch\lib\site-packages\sklearn\metrics\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

=== Interactive Prediction with Uncertainty Estimation and Keyword Analysis ===
Type 'exit' to quit, 'stats' to see feedback statistics, 'retrain' to force retraining

Enter text to classify: hi eto ay isang message na hindi toxic 

=== Prediction with Uncertainty ===
Text: hi eto ay isang message na hindi toxic

Toxicity: TOXIC (Level 1)
Confidence: 0.8978
Uncertainty: 0.0240

Toxicity Features:
  ALL CAPS Usage: 0.00 (0.0% of words)
  Toxic Keywords: 0 (0.0% of words)

Detected Categories:
  - INSULT
    Confidence: 0.8088
    Uncertainty: 0.0279
  - PROFANITY
    Confidence: 0.6048
    Uncertainty: 0.0406

Overall Uncertainty: 0.3677

Detailed Probabilities:
  not toxic: 0.0897
  toxic: 0.8978
  very toxic: 0.0124

Is this prediction correct? (y/n): n
Enter correct toxicity level (0=not toxic, 1=toxic, 2=very toxic): 0
Do you want to provide feedback on categories? (y/n): y
Is 'insult' present? (0=no, 1=yes): 0
Is 'profanity' present? (0=no, 1=yes): 0
Is 'threat' present? (0=no, 1=yes): 0
Is 'identity_hate' present? (0=no, 1=yes): 0
Feedback recorded. Total examples: 1
Need 99 more examples before retraining

Enter text to classify: bakit kataas ng toxic classification kaysa sa ibang classes?

=== Prediction with Uncertainty ===
Text: bakit kataas ng toxic classification kaysa sa ibang classes?

Toxicity: TOXIC (Level 1)
Confidence: 0.9603
Uncertainty: 0.0039

Toxicity Features:
  ALL CAPS Usage: 0.00 (0.0% of words)
  Toxic Keywords: 0 (0.0% of words)

Detected Categories:
  - INSULT
    Confidence: 0.8982
    Uncertainty: 0.0068
  - PROFANITY
    Confidence: 0.7458
    Uncertainty: 0.0106

Overall Uncertainty: 0.1888

Detailed Probabilities:
  not toxic: 0.0303
  toxic: 0.9603
  very toxic: 0.0094

Is this prediction correct? (y/n): n
Enter correct toxicity level (0=not toxic, 1=toxic, 2=very toxic): 0
Do you want to provide feedback on categories? (y/n): y
Is 'insult' present? (0=no, 1=yes): 0
Is 'profanity' present? (0=no, 1=yes): 0
Is 'threat' present? (0=no, 1=yes): 0
Is 'identity_hate' present? (0=no, 1=yes): 0
Feedback recorded. Total examples: 2
Need 98 more examples before retraining

Enter text to classify: fuck you gay bitch hope you DIE now LOSER

=== Prediction with Uncertainty ===
Text: fuck you gay bitch hope you DIE now LOSER

Toxicity: TOXIC (Level 1)
Confidence: 0.9724
Uncertainty: 0.0029

Toxicity Features:
  ALL CAPS Usage: 0.00 (0.0% of words)
  Toxic Keywords: 0 (0.0% of words)

Detected Categories:
  - INSULT
    Confidence: 0.6507
    Uncertainty: 0.0295
  - PROFANITY
    Confidence: 0.6824
    Uncertainty: 0.0218

Overall Uncertainty: 0.1400

Detailed Probabilities:
  not toxic: 0.0055
  toxic: 0.9724
  very toxic: 0.0221

Is this prediction correct? (y/n): m

Enter text to classify: fuck you gay bitch hope you DIE now LOSER

=== Prediction with Uncertainty ===
Text: fuck you gay bitch hope you DIE now LOSER

Toxicity: TOXIC (Level 1)
Confidence: 0.9704
Uncertainty: 0.0040

Toxicity Features:
  ALL CAPS Usage: 0.00 (0.0% of words)
  Toxic Keywords: 0 (0.0% of words)

Detected Categories:
  - INSULT
    Confidence: 0.6444
    Uncertainty: 0.0184
  - PROFANITY
    Confidence: 0.6706
    Uncertainty: 0.0269

Overall Uncertainty: 0.1477

Detailed Probabilities:
  not toxic: 0.0057
  toxic: 0.9704
  very toxic: 0.0238

Is this prediction correct? (y/n): n
Enter correct toxicity level (0=not toxic, 1=toxic, 2=very toxic): 2
Do you want to provide feedback on categories? (y/n): t
Feedback recorded. Total examples: 3
Need 97 more examples before retraining

Enter text to classify: fuck you gay bitch hope you DIE now LOSER

=== Prediction with Uncertainty ===
Text: fuck you gay bitch hope you DIE now LOSER

Toxicity: TOXIC (Level 1)
Confidence: 0.9727
Uncertainty: 0.0037

Toxicity Features:
  ALL CAPS Usage: 0.00 (0.0% of words)
  Toxic Keywords: 0 (0.0% of words)

Detected Categories:
  - INSULT
    Confidence: 0.6566
    Uncertainty: 0.0216
  - PROFANITY
    Confidence: 0.6924
    Uncertainty: 0.0249

Overall Uncertainty: 0.1392

Detailed Probabilities:
  not toxic: 0.0056
  toxic: 0.9727
  very toxic: 0.0217

Is this prediction correct? (y/n): n
Enter correct toxicity level (0=not toxic, 1=toxic, 2=very toxic): 2
Do you want to provide feedback on categories? (y/n): y
Is 'insult' present? (0=no, 1=yes): 1
Is 'profanity' present? (0=no, 1=yes): 1
Is 'threat' present? (0=no, 1=yes): 0
Is 'identity_hate' present? (0=no, 1=yes): 1
Feedback recorded. Total examples: 4
Need 96 more examples before retraining

Enter text to classify: i love you

=== Prediction with Uncertainty ===
Text: i love you

Toxicity: TOXIC (Level 1)
Confidence: 0.9308
Uncertainty: 0.0095

Toxicity Features:
  ALL CAPS Usage: 0.00 (0.0% of words)
  Toxic Keywords: 0 (0.0% of words)

Detected Categories:
  - INSULT
    Confidence: 0.7488
    Uncertainty: 0.0158
  - PROFANITY
    Confidence: 0.5891
    Uncertainty: 0.0215

Overall Uncertainty: 0.2944

Detailed Probabilities:
  not toxic: 0.0479
  toxic: 0.9308
  very toxic: 0.0213

Is this prediction correct? (y/n): n
Enter correct toxicity level (0=not toxic, 1=toxic, 2=very toxic): 0
Do you want to provide feedback on categories? (y/n): y
Is 'insult' present? (0=no, 1=yes): 0
Is 'profanity' present? (0=no, 1=yes): 0
Is 'threat' present? (0=no, 1=yes): 0
Is 'identity_hate' present? (0=no, 1=yes): 0
Feedback recorded. Total examples: 5
Need 95 more examples before retraining

Enter text to classify: what is heppening it can only detect toxic even non toxic is predicted as toxic and the only thing that is being categorize are insult and profanity what should i doo

=== Prediction with Uncertainty ===
Text: what is heppening it can only detect toxic even non toxic is predicted as toxic and the only thing that is being categorize are insult and profanity what should i doo

Toxicity: TOXIC (Level 1)
Confidence: 0.9701
Uncertainty: 0.0034

Toxicity Features:
  ALL CAPS Usage: 0.00 (0.0% of words)
  Toxic Keywords: 0 (0.0% of words)

Detected Categories:
  - INSULT
    Confidence: 0.6971
    Uncertainty: 0.0200
  - PROFANITY
    Confidence: 0.6975
    Uncertainty: 0.0148

Overall Uncertainty: 0.1492

Detailed Probabilities:
  not toxic: 0.0060
  toxic: 0.9701
  very toxic: 0.0239

Is this prediction correct? (y/n): n
Enter correct toxicity level (0=not toxic, 1=toxic, 2=very toxic): 0
Do you want to provide feedback on categories? (y/n): y
Is 'insult' present? (0=no, 1=yes): 0
Is 'profanity' present? (0=no, 1=yes): 0
Is 'threat' present? (0=no, 1=yes): 0
Is 'identity_hate' present? (0=no, 1=yes): 0
Feedback recorded. Total examples: 6
Need 94 more examples before retraining

Enter text to classify: exit
Feedback data saved to feedback_data.pkl

Feedback data saved to feedback_data.pkl

Evaluation complete!
PS E:\final_final_final> 

